{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=384, output_dim=768):\n",
    "        super(EmbeddingProjector, self).__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset[\"train\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][\"image\"]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        embedding = self.dataset[idx][\"embedding_vector\"]\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "        return {\"image\": image, \"embedding\": embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
    "unet = pipeline.unet.to(\"cuda\")\n",
    "vae = pipeline.vae.to(\"cuda\")\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "scheduler = pipeline.scheduler\n",
    "\n",
    "vae.eval()\n",
    "unet.train()\n",
    "\n",
    "#projector = EmbeddingProjector().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.AdamW(list(unet.parameters()) + list(projector.parameters()), lr=1e-5)\n",
    "optimizer = optim.AdamW(unet.parameters(), lr=1e-5)\n",
    "batch_size = 4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Cilem/histopathology\")\n",
    "custom_dataset = CustomDataset(dataset, transform=transform)\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def custom_forward(self, sample, timestep, encoder_hidden_states, cross_attention_kwargs=None):\n",
    "    encoder_hidden_states = projector(encoder_hidden_states)\n",
    "    return self.original_forward(sample, timestep, encoder_hidden_states, cross_attention_kwargs)\n",
    "\n",
    "unet.original_forward = unet.forward\n",
    "unet.forward = custom_forward.__get__(unet) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a histopathology image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"image\"].to(\"cuda\")\n",
    "        text = [prompt] * images.shape[0]\n",
    "        text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        encoder_hidden_states = text_encoder(**text).last_hidden_state\n",
    "        encoder_hidden_states = encoder_hidden_states.to(\"cuda\")\n",
    "        #embeddings = batch[\"embedding\"].to(\"cuda\")  \n",
    "        #projected_embeddings = projector(embeddings)\n",
    "\n",
    "        latents = vae.encode(images).latent_dist.sample() \n",
    "        latents = latents * 0.18215 \n",
    "\n",
    "        noise = torch.randn_like(latents).to(\"cuda\") \n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (latents.shape[0],), device=\"cuda\").long()\n",
    "\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)  \n",
    "        \n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    scheduler=scheduler,\n",
    "    tokenizer=pipeline.tokenizer,\n",
    "    text_encoder=pipeline.text_encoder\n",
    ")\n",
    "pipe.save_pretrained(\"histopathology-diffusion\")\n",
    "\n",
    "# save the projector\n",
    "#torch.save(projector.state_dict(), \"projector.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipe = StableDiffusionPipeline.from_pretrained(\"./histopathology-diffusion_1\")\n",
    "trained_pipe = trained_pipe.to(\"cuda\")\n",
    "\"\"\" # load the projector\n",
    "projector = EmbeddingProjector().to(\"cuda\")\n",
    "projector.load_state_dict(torch.load(\"projector.pth\"))\n",
    "projector.eval() # [B, 768] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" embeddings = next(iter(dataloader))[\"embedding\"].to(\"cuda\")\n",
    "print(embeddings.shape)  # [B, 384]\n",
    "projected_embeddings = projector(embeddings)\n",
    "print(projected_embeddings[0].unsqueeze(0).shape)  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dummy_embed = torch.randn(1, 2, 768).to(\"cuda\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = trained_pipe(\n",
    "    prompt=\"a cat sitting on banch\",\n",
    "    num_images_per_prompt=4,\n",
    "    num_inference_steps=10,\n",
    "    guidance_scale=15\n",
    ").images\n",
    "\n",
    "display(out[0])\n",
    "display(out[1])\n",
    "display(out[2])\n",
    "display(out[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
    "\n",
    "vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(dataset[\"train\"][31500][\"embedding_vector\"])\n",
    "current_length = features.shape[-1]\n",
    "target_length = 768\n",
    "\n",
    "repeat_times = (target_length // current_length) + 1\n",
    "expanded_features = features.repeat(1, repeat_times)[:,:target_length]\n",
    "print(expanded_features.shape)\n",
    "print(features[0][0])\n",
    "print(expanded_features[0][384])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(1, 3, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", safety_checker=None)\n",
    "tokenizer = pipe.tokenizer\n",
    "prompt = \" \"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(tokens)\n",
    "text_encoder = pipe.text_encoder\n",
    "embeds = text_encoder(tokens)[0]\n",
    "print(embeds.shape)\n",
    "dummy = torch.randn(1, 3, 768)\n",
    "# add expanded_features to embeds 1 index\n",
    "dummy[0][0] = embeds[0][0]\n",
    "dummy[0][1] = expanded_features\n",
    "dummy[0][2] = embeds[0][1]\n",
    "\n",
    "\n",
    "\n",
    "pipe.to(\"cuda\", torch.float16)\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(68764)\n",
    "output = pipe(prompt_embeds=dummy,\n",
    "              generator=generator,\n",
    "              num_inference_steps=10,\n",
    "              guidance_scale=8).images[0]\n",
    "\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 00:09:27.511459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-10 00:09:27.542780: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-10 00:09:27.542832: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 00:09:27.567815: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-10 00:09:28.712754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/cilem/anaconda3/envs/ai/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/cilem/anaconda3/envs/ai/lib/python3.10/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.0623376369476318\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, CLIPFeatureExtractor\n",
    "\n",
    "import torch \n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"safety_checker\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "unet = UNet2DConditionModel(\n",
    "    act_fn=\"silu\",\n",
    "    attention_head_dim=8,\n",
    "    center_input_sample=False,\n",
    "    downsample_padding=1,\n",
    "    flip_sin_to_cos=True,\n",
    "    freq_shift=0,\n",
    "    mid_block_scale_factor=1,\n",
    "    norm_eps=1e-05,\n",
    "    norm_num_groups=32,\n",
    "    sample_size=64, \n",
    "    in_channels=4, \n",
    "    out_channels=4, \n",
    "    layers_per_block=2, \n",
    "    block_out_channels=(320, 640, 1280, 1280), \n",
    "    down_block_types=(\n",
    "    \"CrossAttnDownBlock2D\",\n",
    "    \"CrossAttnDownBlock2D\",\n",
    "    \"CrossAttnDownBlock2D\",\n",
    "    \"DownBlock2D\"), \n",
    "    up_block_types=(\"UpBlock2D\",\n",
    "    \"CrossAttnUpBlock2D\",\n",
    "    \"CrossAttnUpBlock2D\",\n",
    "    \"CrossAttnUpBlock2D\"),\n",
    "    cross_attention_dim=384\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02)\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "text_embeddings = torch.randn(1, 1, 384).to(device) \n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "epochs = 1\n",
    "\n",
    "dummy_images = torch.randn(1, 3, 512, 512).to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    images = dummy_images.to(device)\n",
    "    latents = vae.encode(images).latent_dist.sample()\n",
    "\n",
    "    noise = torch.randn_like(latents)\n",
    "    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],), device=device)\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    pred_noise = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(pred_noise, noise)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cilem/anaconda3/envs/ai/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:221: FutureWarning: The configuration file of this scheduler: DDPMScheduler {\n",
      "  \"_class_name\": \"DDPMScheduler\",\n",
      "  \"_diffusers_version\": \"0.32.2\",\n",
      "  \"beta_end\": 0.02,\n",
      "  \"beta_schedule\": \"linear\",\n",
      "  \"beta_start\": 0.0001,\n",
      "  \"clip_sample\": true,\n",
      "  \"clip_sample_range\": 1.0,\n",
      "  \"dynamic_thresholding_ratio\": 0.995,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"rescale_betas_zero_snr\": false,\n",
      "  \"sample_max_value\": 1.0,\n",
      "  \"steps_offset\": 0,\n",
      "  \"thresholding\": false,\n",
      "  \"timestep_spacing\": \"leading\",\n",
      "  \"trained_betas\": null,\n",
      "  \"variance_type\": \"fixed_small\"\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/cilem/anaconda3/envs/ai/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:234: FutureWarning: The configuration file of this scheduler: DDPMScheduler {\n",
      "  \"_class_name\": \"DDPMScheduler\",\n",
      "  \"_diffusers_version\": \"0.32.2\",\n",
      "  \"beta_end\": 0.02,\n",
      "  \"beta_schedule\": \"linear\",\n",
      "  \"beta_start\": 0.0001,\n",
      "  \"clip_sample\": true,\n",
      "  \"clip_sample_range\": 1.0,\n",
      "  \"dynamic_thresholding_ratio\": 0.995,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"rescale_betas_zero_snr\": false,\n",
      "  \"sample_max_value\": 1.0,\n",
      "  \"steps_offset\": 1,\n",
      "  \"thresholding\": false,\n",
      "  \"timestep_spacing\": \"leading\",\n",
      "  \"trained_betas\": null,\n",
      "  \"variance_type\": \"fixed_small\"\n",
      "}\n",
      " has not set the configuration `clip_sample`. `clip_sample` should be set to False in the configuration file. Please make sure to update the config accordingly as not setting `clip_sample` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline(\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    "    scheduler=noise_scheduler,\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_encoder,\n",
    "    safety_checker=safety_checker,\n",
    "    feature_extractor=feature_extractor\n",
    ")\n",
    "\n",
    "pipe.save_pretrained(\"my-diffusion-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27be7a60cae74821a029882ccd412ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cefdd65b9849e589ccc02699710e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAA60lEQVR4Ae3QgQAAAACAoP2pFymECgMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYGDAgAABMLx7WgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"my-diffusion-model\")\n",
    "\n",
    "dummy_prompt = torch.randn(1, 1, 384).to(\"cuda:2\")\n",
    "output = pipeline(\n",
    "    prompt_embeds=dummy_prompt,\n",
    "    num_images_per_prompt=1,\n",
    "    num_inference_steps=10,\n",
    "    guidance_scale=0\n",
    ").images\n",
    "\n",
    "display(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
