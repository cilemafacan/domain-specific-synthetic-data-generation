{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from utils import *\n",
    "from datasets import load_dataset\n",
    "import huggingface_hub as hf_hub\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "hf_hub.login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://huggingface.co/datasets/Cilem/histopathology/resolve/main/histopathology.pkl\n",
    "dataset = load_dataset(\"Cilem/histopathology\")\n",
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
    "\n",
    "vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(dataset[\"train\"][31500][\"embedding_vector\"])\n",
    "current_length = features.shape[-1]\n",
    "target_length = 768\n",
    "\n",
    "repeat_times = (target_length // current_length) + 1\n",
    "expanded_features = features.repeat(1, repeat_times)[:,:target_length]\n",
    "print(expanded_features.shape)\n",
    "print(features[0][0])\n",
    "print(expanded_features[0][384])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(1, 3, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", safety_checker=None)\n",
    "tokenizer = pipe.tokenizer\n",
    "prompt = \" \"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(tokens)\n",
    "text_encoder = pipe.text_encoder\n",
    "embeds = text_encoder(tokens)[0]\n",
    "print(embeds.shape)\n",
    "dummy = torch.randn(1, 3, 768)\n",
    "# add expanded_features to embeds 1 index\n",
    "dummy[0][0] = embeds[0][0]\n",
    "dummy[0][1] = expanded_features\n",
    "dummy[0][2] = embeds[0][1]\n",
    "\n",
    "\n",
    "\n",
    "pipe.to(\"cuda\", torch.float16)\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(68764)\n",
    "output = pipe(prompt_embeds=dummy,\n",
    "              generator=generator,\n",
    "              num_inference_steps=10,\n",
    "              guidance_scale=8).images[0]\n",
    "\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"medium.png\")).unsqueeze(0).to(device)\n",
    "print(image.shape)\n",
    "text = clip.tokenize([\"a diagram\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    print(image_features.shape)\n",
    "    text_features = model.encode_text(text)\n",
    "    print(text_features.shape)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"medium.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    print(image_features[0][:10])\n",
    "    print(text_features[0][:10])\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
