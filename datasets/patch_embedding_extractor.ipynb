{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import openslide as ops\n",
    "import numpy as np\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from huggingface_hub import from_pretrained_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSI_DIR = \"./wsi\"\n",
    "MODEL_NAME = \"/home/cilem/.cache/huggingface/hub/models--google--path-foundation/snapshots/fd6a835ceaae15be80db6abd8dcfeb86a9287e72\"\n",
    "PATCH_DIR = \"./embeddings\"\n",
    "LOG_NAME = \"embedding_extractor.log\"\n",
    "PATCH_SIZE = 1024\n",
    "OVERLAP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    filename=\"./logs/{}\".format(LOG_NAME))\n",
    "\n",
    "logging.info(\"Starting patch extraction...\")\n",
    "if not os.path.exists(PATCH_DIR):\n",
    "    os.makedirs(PATCH_DIR)\n",
    "\n",
    "logging.info(\"Extracting patches from WSI...\")\n",
    "logging.info(\"Patch size: {}\".format(PATCH_SIZE))\n",
    "logging.info(\"Overlap: {}\".format(OVERLAP))\n",
    "logging.info(\"WSI directory: {}\".format(WSI_DIR))\n",
    "logging.info(\"Patch directory: {}\".format(PATCH_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddingExtractor:\n",
    "    def __init__(self, slide_root_path, model_name, patch_size, overlap):\n",
    "        logging.info(f\"Loading model: {model_name}\")\n",
    "        self.model = keras.layers.TFSMLayer(model_name, call_endpoint='serving_default')\n",
    "        self.infer = self.model#.signatures[\"serving_default\"]\n",
    "        \n",
    "\n",
    "        self.slides_path = []\n",
    "        for root, dirs, files in os.walk(slide_root_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.tif'):\n",
    "                    self.slide_path = os.path.join(root, file)\n",
    "                    self.slides_path.append(self.slide_path)\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def extract_patch_embeddings(self):\n",
    "        self.embeddings = []\n",
    "        for slide_path in self.slides_path:\n",
    "            try:\n",
    "                slide = ops.OpenSlide(slide_path)\n",
    "                slide_name = os.path.basename(slide_path)\n",
    "                slide_width, slide_height = slide.dimensions\n",
    "                patch_width, patch_height = self.patch_size\n",
    "                overlap_width, overlap_height = self.overlap\n",
    "\n",
    "                for y in range(0, slide_height, patch_height-overlap_height):\n",
    "                    for x in range(0, slide_width, patch_width-overlap_width):\n",
    "                        patch = slide.read_region(location=(x, y), level=0, size=self.patch_size)\n",
    "                        if patch.size < self.patch_size:\n",
    "                            continue\n",
    "                        else:\n",
    "                            patch_ = patch.convert(\"RGB\")\n",
    "                            patch = patch_.resize((224, 224))\n",
    "                            patch = np.array(patch)\n",
    "                            img = tf.cast(patch, tf.float32) / 255.0\n",
    "                            img = tf.expand_dims(img, 0)\n",
    "                            embedding = self.infer(img)[\"output_0\"].numpy()\n",
    "                            self.embeddings.append({\n",
    "                                \"slide_name\": slide_name,\n",
    "                                \"x\": x,\n",
    "                                \"y\": y,\n",
    "                                \"level\": 0,\n",
    "                                \"patch_size\": self.patch_size,\n",
    "                                \"resize\": (224, 224),\n",
    "                                \"embedding_vector\": embedding\n",
    "                            })\n",
    "                            logging.info(f\"Extracted patch embedding from {slide_path} at ({x}, {y})\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error extracting patch embeddings from {slide_path}: {e}\")\n",
    "\n",
    "        return self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = PatchEmbeddingExtractor(slide_root_path=WSI_DIR, \n",
    "                                    model_name= MODEL_NAME,\n",
    "                                    patch_size=(PATCH_SIZE, PATCH_SIZE), \n",
    "                                    overlap=(OVERLAP, OVERLAP))\n",
    "\n",
    "embeddings = extractor.extract_patch_embeddings()\n",
    "logging.info(\"Number of extracted patches: {}\".format(len(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Saving embeddings...\")\n",
    "with open(os.path.join(PATCH_DIR, f\"embeddings_{PATCH_SIZE}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "from huggingface_hub import hf_hub_download, from_pretrained_keras, login\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "\n",
    "# Download a test image from Hugging Face Hub\n",
    "hf_hub_download(repo_id=\"google/path-foundation\", filename='Test.png', local_dir='.')\n",
    "\n",
    "# Open the image, crop it to match expected input size.\n",
    "img = PILImage.open(\"Test.png\").crop((0, 0, 224, 224)).convert('RGB')\n",
    "\n",
    "# Convert the image to a Tensor and scale to [0, 1] (in case needed)\n",
    "tensor = tf.cast(tf.expand_dims(np.array(img), axis=0), tf.float32) / 255.0\n",
    "\n",
    "# Load the model directly from Hugging Face Hub\n",
    "loaded_model = from_pretrained_keras(\"google/path-foundation\")\n",
    "\n",
    "# Call inference\n",
    "infer = loaded_model.signatures[\"serving_default\"]\n",
    "embeddings = infer(tf.constant(tensor))\n",
    "\n",
    "# Extract the embedding vector\n",
    "embedding_vector = embeddings['output_0'].numpy().flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
